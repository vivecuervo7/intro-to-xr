# Notes

Some notes taken while following these Unity tutorials

<ul>
    <li><a href='https://learn.unity.com/course/introduction-to-xr-vr-ar-and-mr-foundations'>https://learn.unity.com/course/introduction-to-xr-vr-ar-and-mr-foundations</a></li>
    <li><a href='https://learn.unity.com/tutorial/getting-started-with-vr'>https://learn.unity.com/tutorial/getting-started-with-vr</a></li>
    <li><a href='https://learn.unity.com/tutorial/11-user-interface'>https://learn.unity.com/tutorial/11-user-interface</a></li>
    <li><a href='https://learn.unity.com/tutorial/vr-best-practice'>https://learn.unity.com/tutorial/vr-best-practice</a></li>
    <li><a href='https://learn.unity.com/tutorial/optimizing-your-vr-ar-experiences'>https://learn.unity.com/tutorial/optimizing-your-vr-ar-experiences</a></li>
    <li><a href='https://learn.unity.com/project/vr-beginner-the-escape-room'>https://learn.unity.com/project/vr-beginner-the-escape-room</a></li>
    <li><a href='https://learn.unity.com/course/build-a-full-construction-site-tour'>https://learn.unity.com/course/build-a-full-construction-site-tour</a></li>
</ul>

---

## Intro to XR

<details>
    <summary>Solid State IMU</summary>
    <ul>
        <li>Provides reliable rotation tracking information</li>
        <li>Contains accelerometer, gyroscopes and magnetometer</li>
        <li>Can be suseptible to drift due to error accumulation</li>
    </ul>
</details>

<details>
    <summary>Spatialized Audio</summary>
    <ul>
        <li>Mixes sound for each ear based on tracking information</li>
    </ul>
</details>

<details>
	<summary>Motion to Photon Latency</summary>
    <ul>
        <li>The time between moving your head, and seeing the virtual world move as a result</li>
        <li>Any lag between a user's movement and the virtual world reacting to it can result in motion sickness</li>
    </ul>
</details>
<details>
    <summary>Barrel Distortion</summary>
    <ul>
        <li>GPU based shaders are used to perform barrel distortion on the 3D view, so the eye's field of view can be compressed into half a phone screen</li>
        <li>This negates the pincushion distortion introduced by the lenses, effectively restoring the original field of view</li>
        <li>A side effect of this is that the pixels in the center of the view are different sizes to those at the edges, resulting in peripheral vision being more pixelated</li>
    </ul>
</details>
<details>
    <summary>3DOF vs 6DOF</summary>
    <ul>
        <li>6DOF still uses information from accelerometers to do a very fast positional calculation</li>
        <li>Also uses optical detection in a machine vision process to perform periodic corrections</li>
        <li>This enables tracking of a headset or VR controller accurately and prevents drift</li>
    </ul>
</details>

<details>
    <summary>Controller Tracking</summary>
    <ul>
        <li>VR controllers usually use bluetooth to communicate acceleraometer tracking information</li>
        <li>On mobile platforms, 3DOF controller tracking information is all that's available</li>
        <li>In this case, the VR system uses an arm model to calculate positional information based on the orientation of the controller</li>
        <li>Positional information generated by an arm model is inaccurate and only allows limited controller movements</li>
        <li>For roomscale VR, an arm model is not needed because machine vision is used to determine the position of the controllers accurately</li>
    </ul>
</details>

<details>
    <summary>Roomscale VR</summary>
    <ul>
        <li>Roomscale VR means that you can physically move around within the environment, and the viewpoint in virtual reality will both look and move within the 3D environment</li>
    </ul>
</details>

<details>
    <summary>Seated vs Standing VR Experience</summary>
    <ul>
        <li>Seated VR experiences are designed assuming the user is seated, and allows movement using a controller without having to physically move</li>
        <li>A standing VR experience takes advantage of positional tracking and builds it into the design of the experience by encouraging the user to physically move around</li>
    </ul>
</details>

<details>
    <summary>Outside-in VS Inside-out Tracking</summary>
    <ul>
        <li>Outside-in tracking uses external cameras that are looking inward at the user</li>
        <li>Inside-out tracking uses cameras mounted directly on the headset</li>
    </ul>
</details>

---

## VR Museum - Project

Pretty straightforward, some small rendering issues with TextMeshPro which were easy to fix in some cases or weren't worth investing a great deal of time into.

I decided to run the project for Oculus Quest (the tutorial is geared towards Google Cardboard which I don't own) and found the only way I could interact with the scene was to run it in play mode (via Oculus Link) and while resting the gaze reticule over the switches, using the mouse on my PC to trigger a "click".

---

## Vuforia - Project

Ran into a small issue with scaling, following Unity's tutorial resulted in the model being significantly larger than it should have been. This caused the camera to be situated inside the gameObject and took a little bit of head scratching before realizing what had happened. Changing the scale of the prefab to 0.1 caused it to be rendered at a similar size to what was displayed in the tutorial.

The target image can be displayed on a monitor instead of being printed, and should work fine, although with the scaling issue I thought perhaps this was causing the target image to not be tracked properly until I dug a little deeper.

---

## Scene Hierarchies for VR

The following scene hierarchy is recommended in Unity. The Main Camera, Left Hand and Right Hand nodes all use a Tracked Pose Driver to track positions. These do not need a reference transform as they're provided by the object hierarchy.

<ul>
    <li>XR Rig
        <ul>
            <li>Floor Offset
                <ul>
                    <li>Main Camera</li>
                    <li>Left Hand</li>
                    <li>Right Hand</li>
                </ul>
            </li>
        </ul>
    </li>
</ul>

---

## Best Practices

<details>
    <summary>Stereo Rendering Modes</summary>
    <ul>
        <li>Using Single Pass Instanced or Single Pass will allow for performance gains on the CPU and GPU</li>
    </ul>
</details>

<details>
    <summary>Batching <a href='https://docs.unity3d.com/Manual/DrawCallBatching.html?_ga=2.187174696.1042821229.1604531176-103773187.1602014825'>(Unity Docs)</a></summary>
    <ul>
        <li>Batching can reduce work on the GPU</li>
        <li>Relies on having shared Materials and does have some implications on memory and the CPU</li>
    </ul>
</details>

<details>
    <summary>GPU Instancing <a href='https://docs.unity3d.com/Manual/GPUInstancing.html?_ga=2.54480680.1042821229.1604531176-103773187.1602014825'>(Unity Docs)</a></summary>
    <ul>
        <li>Rendering multiple copies of the same mesh at once using a smaller number of draw calls</li>
        <li>Useful for drawing objects that appear repeatedly in a scene</li>
    </ul>
</details>

<details>
    <summary>Lighting Strategy <a href='https://unity3d.com/learn/tutorials/projects/creating-believable-visuals/lighting-strategy?_ga=2.123104139.1042821229.1604531176-103773187.1602014825'>(Unity Docs)</a></summary>
    <ul>
        <li>Using fully realtime lighting or realtime global illumination is not recommended due to performance</li>
        <li>Instead, use non-directional lightmaps for static objects and light probes for dynamic objects</li>
        <li>The Unity Docs link identifies two ideal strategies, of which Mixed Lighting with Shadowmask + Light Probe is likely to be the best option as it allows for shadows for dynamic objects</li>
    </ul>
</details>

<details>
    <summary>Cameras</summary>
    <ul>
        <li>The camera's orientation and position should always respond to the user's motion</li>
        <li>Moving the camera without user interaction can result in motion sickness, such as head bobbing, camera shake and cinematic cameras</li>
        <li>Field of view is disallowed. The VR SDKs will provide the stereo projection matrices</li>
        <li>Depth of field or motion blur should be avoided as they can lead to simulation sickness</li>
        <li>Moving or rotating the horizon line or other large components should be avoided to not affect the user's sense of stability</li>
        <li>Set the near clip plane for 1st person cameras to the minimal acceptable value for correct rendering of objects</li>
        <li>Set the far clip plane to a value that optimizes frustum culling</li>
        <li>For UI, prefer world space over screen space Canvas</li>
    </ul>
</details>

<details>
    <summary>Post-Processing</summary>
    <ul>
        <li>This can become expensive very quickly in VR</li>
        <li>Some effects (depth of field, motion blur) should be avoided due to inducing sickness</li>
        <li>Using the Post-Processing Stack can merge several enabled effects into fewer passes to improve performance</li>
    </ul>
</details>

<details>
    <summary>Anti-aliasing</summary>
    <ul>
        <li>Highly desirable in VR</li>
        <li>Forward Rendering supplies MSAA which can be enabled in the Quality Settings</li>
    </ul>
</details>

<details>
    <summary>Shaders</summary>
    <ul>
        <li>Review shaders generated by visual shader editors (Shader Graph etc) and optimize where possible</li>
        <li>Use a Shader Variant Collection and call its WarmUp function at an opportune time (loading screen etc) to mitigate frame rate hiccups when a shader is first used</li>
        <li>Some shaders may require modifications when using Single-Pass Stereo Rendering</li>
    </ul>
</details>

<details>
    <summary>Render Scale</summary>
    <ul>
        <li>Changing XRSettings.eyeTextureResolutionScale and XRSettings.renderViewportScale can adjust rendered scene quality</li>
        <li>Setting XRSettings.eyeTextureResolutionScale below 1.0 will decrease image quality whereas setting it above 1.0 will lead to a crisper image</li>
        <li>Changing XRSettings.eyeTextureResolutionScale will cause a hitch, so it would be prudent to pick an opportune time to adjust it</li>
    </ul>
</details>

<details>
    <summary>Motion-to-Photon Latency</summary>
    <ul>
        <li>Recommended to target 20 milliseconds or less</li>
        <li>Avoid handling input in FixedUpdate as this doesn't necessarily get called once per frame and may excacerbate input latency</li>
        <li>Tracked poses may be handled twice (in Update and onBeforeRender). Any additional handling in onBeforeRender should be very lightweight</li>
    </ul>
</details>

<details>
    <summary>Mobile Considerations</summary>
    <ul>
        <li>Minimize or eliminate the use of post-processing</li>
        <li>Keep geometry as simple as possible</li>
        <li>Minimize draw calls</li>
        <li>Avoid using Standard Shader or other computation-heavy shaders</li>
        <li>Physics is computation-heavy. Where possible, replace Physics colliders with distance-checking logic</li>
        <li>Consider setting Sustained Performance Mode to mitigate thermal throttling where required</li>
    </ul>
</details>

---

## Optimizing XR Experiences

<details>
    <summary>Light Baking</summary>
    <ul>
        <li>This is pre-calculating scene lighting to to eliminate run-time overhead</li>
        <li>Normally, Unity renders each object for every light shining on it (an object with 5 lights shining on it will be rendered 5 times)</li>
    </ul>
</details>

<details>
    <summary>Occlusion Culling</summary>
    <ul>
        <li>Unity by default won't render objects outside the viewing frustum, but will continue to render objects that aren't visible due to other objects</li>
        <li>Using occlusion culling will also prevent rendering of objects blocked by other objects in the foreground</li>
    </ul>
</details>

<details>
    <summary>Static Batching</summary>
    <ul>
        <li>Marking all stationary objects as static will allow Unity to combine them into a single mesh, rendering them faster</li>
        <li>This also removes them from physics calculations as Unity knows these objects won't move</li>
        <li>This can help when an application is CPU-bound</li>
    </ul>
</details>

---

## Light Baking

- Set all non-moving objects in the scene to Static to include them in lighting calculations
- Change each of the scene's light modes to Baked
- Under Lighting settings, click Generate Lighting
- When complete, all baked lightmaps will be visible in the Global Maps and Object Maps tabs of the Lighting window
- Adjusting the lightmap size will affect accuracy, bake time and build file size

---

## Occlusion Culling

- Tag all objects occluding other objects as Occluder Static and all small occluded objects as Occludee Static
- If rendering a window, it should be tagged as Occludee Static
- Open the Occlusion window (Window > Rendering > Occlusion Culling)
- Under the Bake tab, click Bake

---

## Static Batching

- Open Player Settings (File > Build Settings > Player Settings)
- Enable Static Batching
- In the hierarchy, select all non-moving objects and tag them as Batching Static
- Note: Toggling the Static checkbox should enable _all_ the static tags, and is the common practice

---

## VR Escape Room - Project

<details>
    <summary>Spatialized Audio</summary>
    <ul>
        <li>Make sure sounds have their Audio Source Spatial Blend set to 3D</li>
    </ul>
</details>

<details>
    <summary>UI Development with the XR Interaction Toolkit</summary>
    <ul>
        <li>Need a GameObject with an XR Ray Interactor component</li>
        <li>Need a normal UI Canvas with a Tracked Device Graphic Raycaster component</li>
        <li>Need an XR UI Input Module added to the EventSystem created when adding a Canvas - make sure to remove the Standard Input Module</li>
    </ul>
</details>

---

## VR Construction Site - Project

Can't say I was the biggest fan of this tutorial series, the bite-sized chunks are good for delivering pieces of information but there is a lot of content targeted towards beginners that needs to be sorted through. The tutorials don't follow on from one another, so there is a lot of downloading new projects and configuring them for the target device for a short piece of content.

The tutorials make use of the OVRInteractCameraRig, which comes with the Oculus Go controller. This isn't mentioned until a couple of tutorials deep into the VR section of the series. I'm using an Oculus Quest, so needed to use the OVRCameraRig.

I found this tutorial series to go from walking you through some very basic things, to handling more complex scenarios such as locomotion by handing you a script with instructions to drop them on certain objects. In the latter sections, I ran into a few issues where being developed for Oculus Go, or sometimes just outdated scripts meant it would have required some heavy tweaking to their supplied scripts to have it work with Oculus Quest controllers, and with the bite-sized chunks of work it didn't seem worth the hassle.

The last section on optimization and polishing the scene felt a lot more valuable than the earlier sections.

<details>
    <summary>Materials</summary>
    <ul>
        <li>Online tool for generating normal maps <a href='http://cpetry.github.io/NormalMap-Online/'>http://cpetry.github.io/NormalMap-Online/</a></li>
    </ul>
</details>

<details>
    <summary>CAD Workflow</summary>
    <ul>
        <li>Unity was crashing when trying to import the more complex models, so while I did get to run through using the plugin, I wasn't able to complete the entire section</li>
    </ul>
</details>

<details>
    <summary>Timelines</summary>
    <ul>
        <li>Use Timelines to tie together multiple smaller animations and other actions like sounds or activating objects</li>
    </ul>
</details>

<details>
    <summary>UI Development</summary>
    <ul>
        <li>Similar to the last project, on Oculus we need to replace EventSystem's Standard Input Module with OVR Input Module</li>
        <li>To select UI components with your gaze, set OVR Input Module's Ray Transform to CenterEyeAnchor</li>
        <li>Replace the Canvas' Graphic Raycaster component with the OVR Raycaster component</li>
        <li>Set the Canvas' Event Camera field to CenterEyeAnchor</li>
    </ul>
</details>

---

## VR Construction Site: Reflections and Lighting

<details>
    <summary>Color Space</summary>
    <ul>
        <li>Linear and gamma color space differ, so it's best to make this decision early on</li>
        <li>Linear tends to be the preferred color space, and is considered more accurate but can impact performance</li>
        <li>Textures tend to be saved in gamma color space, while shaders expect linear color space</li>
    </ul>
</details>

<details>
    <summary>Rendering Path</summary>
    <ul>
        <li>Forward Rendering is recommended for mobile VR due to performance</li>
        <li>Deferred Rendering has the most lighting and shadow fidelity, best suited to scenes with many realtime lights</li>
        <li>Deferred Rendering loses MSAA, but you can add anti-aliasing back with the Post Processing Stack</li>
    </ul>
</details>

<details>
    <summary>Reflection Probes</summary>
    <ul>
        <li>Can be quite performance heavy, ideally should use baked probes unless realtime is absolutely necessary</li>
        <li>Allows for reflective materials to properly reflect the surrounding environment</li>
    </ul>
</details>

<details>
    <summary>Baked Lightmaps</summary>
    <ul>
        <li>Baking lightmaps can increase lighting accuracy and application performance</li>
        <li>It should be used where possible, but dynamic objects will need some form of realtime lighting</li>
        <li>Dynamic objects can still receive global illumination from static objects via Light Probes</li>
    </ul>
</details>

---

## VR Construction Site: Audio Accuracy and Realism

<details>
    <summary>Spatializer Plugin</summary>
    <ul>
        <li>The tutorial focuses on Steam Audio</li>
        <li>Set the aptializer plugin in the project settings (Edit > Project Settings > Audio)</li>
        <li>Must be set up to work with your audio engine (in this case, Unity's built-in audio engine)</li>
        <li>Make sure to use 2D Spatial Blend if using the plugin's Physics Based Attenuation, as it is applied over the top</li>
        <li>Scene must be pre-exported after setting up audio to use the Steam Audio plugin (Window > Steam Audio > Pre-Export Scene)</li>
    </ul>
</details>

<details>
    <summary>Sound Occlusion and Reflections</summary>
    <ul>
        <li>The spatializer plugin can perform physics based sound occlusion and reflections</li>
        <li>Add a Steam Audio Geometry component to add itself and its children as audio geometry</li>
        <li>These can be baked, similar to lightmaps, to pre-compute audio geometry to alleviate runtime overhead</li>
    </ul>
</details>

---

## VR Construction Site: Post-Processing

<details>
    <summary>Post Processing Volume</summary>
    <ul>
        <li>Setting isGlobal will make the effects apply to the entire scene</li>
        <li>When not using isGlobal, it becomes a local volume. A collider or trigger is needed to define its boundaries</li>
        <li>It is recommended to use simple colliders as meshes can be quite expensive</li>
    </ul>
</details>
